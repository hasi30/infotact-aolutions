# -*- coding: utf-8 -*-
"""infotACT project 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ter7exk-DmyrrrKc5tf2hJT2Z5mCGLTJ

WEEK 1 tasks
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load your datasets
df_tasks = pd.read_csv("Task Catagories.csv.csv")
df_projects = pd.read_excel("project Management.xlsx", sheet_name='data')

# Sample task descriptions randomly to match project rows
sampled_tasks = df_tasks.sample(n=len(df_projects), replace=True).reset_index(drop=True)

# Merge them by adding task details to the projects
df_projects['Task Description'] = sampled_tasks['Task Description']
df_projects['Category'] = sampled_tasks['Category']
df_projects['Skill'] = sampled_tasks['Skill']

# Save the combined dataset
df_projects.to_csv("Combined_Task_Management.csv", index=False)
# === Handle Missing Values ===
df_projects.fillna("Unknown", inplace=True)
df_projects.head()

# View Dataset Overview
print(df_projects.head())
print(df_projects.info())
print(df_projects.describe(include='all'))

# Check for Missing Values
print("\nMissing Values:\n", df_projects.isnull().sum())

# Check for Duplicates
print("\nDuplicate Rows: ", df_projects.duplicated().sum())

# Remove Duplicates
df = df_projects.drop_duplicates()

median_date = pd.to_datetime(df['COmpleteDate'][df['COmpleteDate'] != 'Unknown'], errors='coerce', format='%d/%m/%Y').median() # Added format='%d/%m/%Y'

# Replace missing dates with the median
df['COmpleteDate'] = df['COmpleteDate'].replace('Unknown', median_date)

# Convert to datetime if needed
df['COmpleteDate'] = pd.to_datetime(df['COmpleteDate'], errors='coerce', format='%d/%m/%Y') # Added format='%d/%m/%Y'

# Rename the 'COmpleteDate' column to 'deadline'
df = df.rename(columns={'COmpleteDate': 'deadline'})

# Print 15 rows of the 'deadline' column
print(df['deadline'].head(15))

df.columns

# Convert 'EntryDate' to datetime, assuming it's currently in string format
# Adjust the format string '%d/%m/%Y' if your EntryDate has a different format
df['EntryDate'] = pd.to_datetime(df['EntryDate'], format='%d/%m/%Y', errors='coerce')

# Calculate the median date (excluding NaT values)
median_entry_date = pd.to_datetime(df['EntryDate'][df['EntryDate'].notnull()], errors='coerce').median()

# Format the median date to 'dd/mm/yyyy'
formatted_median_date = median_entry_date.strftime('%d/%m/%Y')

# Replace NaT values with the formatted median date
df['EntryDate'] = df['EntryDate'].fillna(pd.to_datetime(formatted_median_date, format='%d/%m/%Y'))

df.head()

from datetime import timedelta

# Example list of urgent keywords
urgent_keywords = ['urgent', 'asap', 'immediate', 'critical', 'important']

# Function to assign priority based on rules
def assign_priority(row):
    # Check for urgent keywords in task description
    if any(keyword in row['Task Description'].lower() for keyword in urgent_keywords):
        return 'high'

    # Check deadline proximity
    days_left = (row['deadline'] - pd.to_datetime('today')).days
    if days_left <= 3:
        return 'high'
    elif days_left <= 7:
        return 'medium'
    else:
        return 'low'

# Apply function to dataframe
df['priority'] = df.apply(assign_priority, axis=1)

# Check result
print(df[['Task Description', 'deadline', 'priority']].head())

df_projects.shape

import seaborn as sns
from wordcloud import WordCloud
import pandas as pd # Import pandas

# Ensure 'deadline' and 'EntryDate' are in datetime format
df['deadline'] = pd.to_datetime(df['deadline'], errors='coerce')
df['EntryDate'] = pd.to_datetime(df['EntryDate'], errors='coerce')

# Create the 'overdue' column
# Compare the 'deadline' with today's date
df['overdue'] = df['deadline'] < pd.to_datetime('today')

# Replace boolean values with string labels
df['overdue'] = df['overdue'].replace({True: 'Overdue', False: 'Upcoming'})

# Extract Month Name for grouping (e.g., 'May')
df['EntryMonth'] = df['EntryDate'].dt.strftime('%B')

# Scatter Plot by EntryMonth and Category
plt.figure(figsize=(12, 6))
sns.scatterplot(data=df, x='EntryMonth', y='Category', hue='WorkStatus', palette='Set1', s=100, alpha=0.7)
plt.title('Task Status over Time by Category (Monthly)')
plt.xlabel('Entry Month')
plt.ylabel('Category')

# Get unique month names and their order in the DataFrame
month_order = df['EntryMonth'].unique()

# Set x-axis ticks and labels to display full month names
plt.xticks(ticks=range(len(month_order)), labels=month_order, rotation=45)

plt.show()

# 2. Task Count per Site Name
plt.figure(figsize=(10,6))
top_sites = df['Site Name'].value_counts().head(10)
sns.barplot(x=top_sites.values, y=top_sites.index, palette='magma')
plt.title('Top 10 Site Names by Task Count')
plt.xlabel('Number of Tasks')
plt.ylabel('Site Name')
plt.show()

# 3. WorkStatus Distribution
plt.figure(figsize=(6,4))
sns.countplot(data=df, x='WorkStatus', palette='Set2')
plt.title('Task Work Status Distribution')
plt.xticks(rotation=45)
plt.show()

# 4. Priority Distribution
plt.figure(figsize=(6,4))
sns.countplot(data=df, x='priority', palette='Set3')
plt.title('Task Priority Distribution')
plt.show()

# 5. Overdue vs Upcoming Tasks
# Pie chart for Overdue vs Upcoming Tasks
overdue_counts = df['overdue'].value_counts()
labels = ['Upcoming', 'Overdue'] # Labels correspond to the possible values in the 'overdue' column

plt.figure(figsize=(6,6))
plt.pie(overdue_counts, labels=labels, autopct='%1.1f%%', startangle=140, colors=['#66b3ff','#ff6666'])
plt.title('Overdue vs Upcoming Tasks')
plt.axis('equal')
plt.show()

# 6. Revenue Distribution (RecurringRevenue and NRR)
fig, axes = plt.subplots(1, 2, figsize=(12,5))

sns.histplot(df['ReccuringRevenue'], kde=True, ax=axes[0], color='teal')
axes[0].set_title('Recurring Revenue Distribution')

sns.histplot(df['NRR'], kde=True, ax=axes[1], color='orange')
axes[1].set_title('Net Recurring Revenue (NRR) Distribution')

plt.tight_layout()
plt.show()

# 7. Task Count per Category
plt.figure(figsize=(10,6))
sns.countplot(data=df, y='Category', order=df['Category'].value_counts().index, palette='cubehelix')
plt.title('Task Count per Category')
plt.show()

# 8. Task Count per Skill
plt.figure(figsize=(10,6))
top_skills = df['Skill'].value_counts().head(10)
sns.barplot(x=top_skills.values, y=top_skills.index, palette='plasma')
plt.title('Top 10 Skills by Task Count')
plt.show()


# 9. Task Owners Distribution
plt.figure(figsize=(10,6))
top_owners = df['Task Owner '].value_counts().head(10)
sns.barplot(x=top_owners.values, y=top_owners.index, palette='spring')
plt.title('Top 10 Task Owners by Number of Tasks')
plt.show()

# 10. WordCloud of Task Descriptions
task_text = ' '.join(df['Task Description'].dropna().astype(str).tolist())

wordcloud = WordCloud(width=800, height=400, background_color='white').generate(task_text)

plt.figure(figsize=(15, 7.5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('WordCloud of Task Descriptions', fontsize=20)
plt.show()

import pandas as pd

# Calculate the mode of the 'Task Owner' column (excluding "Unknown")
mode_owner = df[df['Task Owner '] != 'Unknown']['Task Owner '].mode()[0]

# Replace "Unknown" values with the mode
df['Task Owner '] = df['Task Owner '].replace('Unknown', mode_owner)

# Display the updated DataFrame (optional)
print(df.head())

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

# Download the 'punkt_tab' data
nltk.download('punkt_tab') # Download necessary data for sent_tokenize
nltk.download('stopwords')
nltk.download('punkt')

# Example function to clean and preprocess task descriptions
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters & numbers
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(word) for word in tokens]
    return ' '.join(tokens)

# Apply to DataFrame
df['cleaned_task_description'] = df['Task Description'].apply(preprocess_text)

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()

def preprocess_text(text):
    text = str(text).lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # remove punctuation and numbers
    tokens = text.split()
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)

df['cleaned_desc'] = df['Task Description'].apply(preprocess_text)

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(max_features=100)
tfidf_features = tfidf.fit_transform(df['cleaned_desc'])

# Optional: Convert to DataFrame
tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=tfidf.get_feature_names_out())

from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight & fast

# Generate BERT embeddings
bert_embeddings = model.encode(df['Task Description'].astype(str).tolist())

# Convert to DataFrame (optional)
bert_df = pd.DataFrame(bert_embeddings)

print(df.columns)

from sklearn.feature_extraction.text import TfidfVectorizer

# Clean text if not already done
df['cleaned_desc'] = df['Task Description'].fillna('').apply(preprocess_text)

# Vectorize
tfidf = TfidfVectorizer(max_features=1000)
X = tfidf.fit_transform(df['cleaned_desc'])

# Target variable
y = df[target_col].astype(str)

target_col = ['priority']

# Clean text if not already done
df['cleaned_desc'] = df['Task Description'].fillna('').apply(preprocess_text)

# Vectorize
tfidf = TfidfVectorizer(max_features=1000)
X = tfidf.fit_transform(df['cleaned_desc'])

# Target variable
# Now target_col is a list, which pandas correctly interprets for selecting multiple columns
y = df[target_col].astype(str)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score

nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)
nb_preds = nb_model.predict(X_test)

print("Naive Bayes Classification Report:")
print(classification_report(y_test, nb_preds))

from sklearn.svm import LinearSVC

svm_model = LinearSVC()
svm_model.fit(X_train, y_train)
svm_preds = svm_model.predict(X_test)

print("SVM Classification Report:")
print(classification_report(y_test, svm_preds))